
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>TPAMI Special Issue</title>

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
  <link href="./css/style.css" rel="stylesheet" type="text/css" />
</head>

<body> 

<div class="container">
  <table border="0" align="center">
    <tr>
      <td width="623" align="center" valign="middle"><h2><b>TPAMI Special Issue</b></h2>
      <span class="title"><font color="#FF0000">CALL FOR PAPERS</font></span></td>
    </tr>
    <tr>
        <td colspan="3" align="center"><h1>Large-Scale Multimodal Learning: <br> Universality, Robustness, Efficiency, and Beyond</h1></td>
    </tr>
  </table>
  <p><img src="./figures/TPAMI_logo.png" width="500" align="middle" /></p>
  
</div>

</br>







<div class="container">
  <h2>Guest Editors</h2>
    <div>
      <div class="instructor">
          <a href="http://www.pengxu.net/" >
        <div class="instructorphoto"><img src="figures/portraits/peng.jpg"></div>
        <div>Peng Xu<br>Tsinghua University</div>
        </a>
                
      </div>

      <div class="instructor">
        <a href="https://songbai.site/">
            <div class="instructorphoto"><img src="figures/portraits/songbai.jpg"></div>
            <div>Song Bai<br>ByteDance AI Lab</div>
        </a>
        
      </div>

      <div class="instructor">
        <a href="http://web.ee.tsinghua.edu.cn/zhoubowen/en/index.htm">
            <div class="instructorphoto"><img src="figures/portraits/zhou.jpg"></div>
            <div>Bowen Zhou<br>Tsinghua University</div>
        </a>
        
      </div>

      <div class="instructor">
        <a href="https://eng.ox.ac.uk/people/david-clifton/">
            <div class="instructorphoto"><img src="figures/portraits/david-clifton-provided.jpg"></div>
            <div>David Clifton<br> University of Oxford</div>
        </a>
        
      </div>
      
      
      
      <div class="instructor">
          <a href="https://www.robots.ox.ac.uk/~vedaldi/" >
        <div class="instructorphoto"><img src="figures/portraits/vedaldi.jpg"></div>
        <div>Andrea Vedaldi<br> University of Oxford</div>
        </a>
                
      </div>

      <div class="instructor">
          <a href="https://www.vanderschaar-lab.com/prof-mihaela-van-der-schaar/" >
        <div class="instructorphoto"><img src="figures/portraits/mihaela_a.jpg"></div>
        <div>Mihaela van der Schaar<br> University of Cambridge</div>
        </a>
                
      </div>

      <div class="instructor">
          <a href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html" >
        <div class="instructorphoto"><img src="figures/portraits/lvg.jpeg"></div>
        <div>Luc Van Gool<br> ETH Zurich</div>
        </a>
                
      </div>

      
      
    </div>
 
</div>   

</br>

<div class="container">
  <h2>Overview</h2>
    <p style="text-align:justify; text-justify:inter-ideograph;">Thanks to the development of the internet and a wide variety of intelligent devices in
recent years, increasing amounts of multimodal data are being transmitted over the
internet, thus an increasing number of multimodal application scenarios are emerging. In
modern life, we can see various multimodal applications, including commercial services
(e.g., e-commerce/commodity retrieval, vision-and-language navigation), communication
(e.g., lip reading, sign language translation), human-computer interaction, Healthcare AI,
surveillance AI, etc. Specifically, in the era of Deep Learning, deep neural networks
greatly promote the development of multimodal learning.
    </p>

    <p style="text-align:justify; text-justify:inter-ideograph;">The goal of this special issue is to bring together perspectives from multiple disciplines
(e.g., Computer Vision, Natural Language Processing, Machine Learning, Deep Learning,
Healthcare AI, Medical Image ML, Bioinformatics, Cognitive Science, Sociology) to
highlight major open questions and to identify research opportunities to address
outstanding challenges in the domain of multimodal learning in the Deep Learning and
Big Data Era.
    </p>
</div>

</br>



<div class="container">
  <h2>Call for Papers</h2>
  <p>(CFP poster can be downloaded via this <a href="figures/cfp.png">link</a>)</p>

    <h3>Topics of Interests</h3>

    <div align="center">
      
      
      
      <p style="text-align:justify; text-justify:inter-ideograph;">Topics of interest include, but are not limited to:</p>

      <div align="left">
        <ul>

        <li>universal multimodal learning</li>
        <li>robust multimodal learning</li>
        <li>efficient multimodal learning</li>
        <li>large scale multimodal pre-training</li>
        <li>self-supervised/un-supervised/weakly-supervised multimodal learning</li>
        <li>multimodal representation learning</li>
        <li>multimodal transfer learning</li>
        <li>multimodal metric learning</li>
        <li>multimodal generation</li>
        <li>multimodal dataset and evaluation metric</li>
        <li>multimodal applications</li>


          

        </ul>
      </div>
    </div>


    <h3>Important Dates</h3>
      <div align="left">
        <p><span class="announce_date"> <font color="#FF0000">We received a large number of submissions. So, the timeline had to be significantly delayed due to the heavy workload.</font> </span> </p>
        <p><span class="announce_date"> Paper submission due </span>.  <s>Mar. 1, 2023</s> Apr. 1, 2023 </p>
        <p><span class="announce_date"> First notification </span>. <s>Jun. 1, 2023</s>  </p>
        <p><span class="announce_date"> Revision due </span>. <s>Aug. 1, 2023</s>  </p>
        <p><span class="announce_date"> Final decision </span>. <s>Sep. 1, 2023</s>  </p>
        <p><span class="announce_date"> Publication date (tentative) </span>. <s>Oct. 1, 2023</s>  </p>
      </div>


    <h3>Submission and Review</h3>
    <div align="center">
      <p style="text-align:justify; text-justify:inter-ideograph;">The review process will follow the standard procedures of TPAMI. This special issue will
request the reproducibility of experimental results. We will ask the authors to release their
code upon acceptance and to provide the necessary experimental details.
      </p>

      <p style="text-align:justify; text-justify:inter-ideograph;"> Before
submitting the manuscript, please read the Instructions for Authors for TPAMI
(<a href="https://www.computer.org/web/tpami/author">https://www.computer.org/web/tpami/author</a>).
      </p>

      <p style="text-align:justify; text-justify:inter-ideograph;">The manuscripts should be submitted to:
<a href="https://mc.manuscriptcentral.com/tpami-cs">https://mc.manuscriptcentral.com/tpami-cs</a>
      </p>
      
    </div>


</div>


</br>








<div class="containersmall">
  <p>The webpage template is from <a href="https://gkioxari.github.io/">here</a>.</p>
  <div style="width:256px;">
    <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=cmfpHpIr5bdv0CU_C2k_92Q6PsHu9SKoWJirrqNFEhs&cl=ffffff&w=a"></script>
  </div>
</div>
 
<!--<p align="center" class="acknowledgement">Last updated: 30 July 2012</p>-->
</body>
</html>
